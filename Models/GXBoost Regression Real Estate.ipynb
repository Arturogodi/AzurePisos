{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8566589c-4802-4ef7-acb6-33c5faad561e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../ConfigFolder/ConfigSAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b294718c-c56b-4d97-9880-35d515173bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Notebook Description\n",
    "\n",
    "This notebook provides a comprehensive analysis and visualization of the dataset. It includes data preprocessing, exploratory data analysis, and the implementation of various machine learning models to predict outcomes. The notebook is structured to guide the user through each step, ensuring a clear understanding of the process and results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24987791-c368-47b3-a318-42b30acd3648",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) Load Delta DataFrame\n",
    "model_combined_Path = generate_path('g-model-madrid-min-features-combined', 'goldlayer')\n",
    "df_model_combined = spark.read.format(\"delta\").load(model_combined_Path)\n",
    "\n",
    "# 2) Extract 'year' and 'month' from 'period'\n",
    "from pyspark.sql.functions import col, substring, when\n",
    "\n",
    "df_model_combined = df_model_combined.withColumn(\"year\", substring(col(\"period\"), 1, 4)) \\\n",
    "                                     .withColumn(\"month\", substring(col(\"period\"), 6, 2))\n",
    "\n",
    "# 3) Derive 'quarter' (based on 'month') and drop 'month' and 'period'\n",
    "df_model_combined = df_model_combined.withColumn(\n",
    "    \"quarter\",\n",
    "    when(col(\"month\") == \"3\", \"1\")\n",
    "     .when(col(\"month\") == \"6\", \"2\")\n",
    "     .when(col(\"month\") == \"9\", \"3\")\n",
    "     .when(col(\"month\") == \"2\", \"4\")\n",
    ").drop(\"month\", \"period\")\n",
    "\n",
    "# 4) Cast columns to appropriate types\n",
    "df_cleaned = (\n",
    "    df_model_combined\n",
    "    .withColumn('bathrooms',  col('bathrooms').cast('int'))\n",
    "    .withColumn('isparking',   col('isparkingspaceincludedinprice').cast('int'))\n",
    "    .withColumn('latitude',    col('latitude').cast('float'))\n",
    "    .withColumn('longitude',   col('longitude').cast('float'))\n",
    "    .withColumn('price',       col('price').cast('float'))\n",
    "    .withColumn('rooms',       col('rooms').cast('int'))\n",
    "    .withColumn('size',        col('size').cast('float'))\n",
    "    .withColumn('year',        col('year').cast('int'))\n",
    "    .withColumn('quarter',     col('quarter').cast('int'))\n",
    ")\n",
    "\n",
    "df_cleaned.printSchema()\n",
    "\n",
    "# 5) Create feature vector with VectorAssembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "feature_cols = ['bathrooms', 'isparking', 'latitude', 'longitude', 'rooms', 'size', 'year', 'quarter']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol='features',\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "\n",
    "# 6) Transform DataFrame to obtain 'features' and 'price'\n",
    "df_prepared = assembler.transform(df_cleaned).select('features', 'price')\n",
    "\n",
    "# 7) Split into training and test sets\n",
    "train_data, test_data = df_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 8) Train Gradient Boosting model\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "xgb = GBTRegressor(featuresCol='features', labelCol='price', maxIter=100, maxDepth=5)\n",
    "\n",
    "with mlflow.start_run(run_name=\"boost_model_v1\") as run:\n",
    "    # Train\n",
    "    xgb_model = xgb.fit(train_data)\n",
    "    \n",
    "    # Predict\n",
    "    xgb_predictions = xgb_model.transform(test_data)\n",
    "    \n",
    "    # Evaluate with RMSE, MAE, and R2\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "    evaluator_mae  = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='mae')\n",
    "    evaluator_r2   = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='r2')\n",
    "    \n",
    "    rmse_xgb = evaluator_rmse.evaluate(xgb_predictions)\n",
    "    mae_xgb  = evaluator_mae.evaluate(xgb_predictions)\n",
    "    r2_xgb   = evaluator_r2.evaluate(xgb_predictions)\n",
    "    \n",
    "    # Log metrics in MLflow\n",
    "    mlflow.log_metric(\"rmse_xgb\", rmse_xgb)\n",
    "    mlflow.log_metric(\"mae_xgb\",  mae_xgb)\n",
    "    mlflow.log_metric(\"r2_xgb\",   r2_xgb)\n",
    "    \n",
    "    mlflow.log_param(\"seed\", 42)\n",
    "    mlflow.log_param(\"feature_cols\", feature_cols)\n",
    "    mlflow.log_param(\"maxIter\", 100)\n",
    "    mlflow.log_param(\"maxDepth\", 5)\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.spark.log_model(xgb_model, \"gbt-model\")\n",
    "\n",
    "    print(f\"[GBT]  RMSE={rmse_xgb}, MAE={mae_xgb}, R2={r2_xgb}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GXBoost Regression Real Estate",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
