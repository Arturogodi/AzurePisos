{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8566589c-4802-4ef7-acb6-33c5faad561e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../ConfigFolder/ConfigSAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24987791-c368-47b3-a318-42b30acd3648",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) Cargar DataFrame delta\n",
    "model_combined_Path = generate_path('g-model-madrid-min-features-combined', 'goldlayer')\n",
    "df_model_combined = spark.read.format(\"delta\").load(model_combined_Path)\n",
    "\n",
    "# 2) Extraer 'year' y 'month' de 'period'\n",
    "from pyspark.sql.functions import col, substring, when\n",
    "\n",
    "df_model_combined = df_model_combined.withColumn(\"year\", substring(col(\"period\"), 1, 4)) \\\n",
    "                                     .withColumn(\"month\", substring(col(\"period\"), 6, 2))\n",
    "\n",
    "# 3) Derivar 'quarter' (basado en 'month') y eliminar 'month' y 'period'\n",
    "df_model_combined = df_model_combined.withColumn(\n",
    "    \"quarter\",\n",
    "    when(col(\"month\") == \"3\", \"1\")\n",
    "     .when(col(\"month\") == \"6\", \"2\")\n",
    "     .when(col(\"month\") == \"9\", \"3\")\n",
    "     .when(col(\"month\") == \"2\", \"4\")\n",
    ").drop(\"month\", \"period\")\n",
    "\n",
    "# 4) Castear columnas al tipo adecuado\n",
    "df_cleaned = (\n",
    "    df_model_combined\n",
    "    .withColumn('bathrooms',  col('bathrooms').cast('int'))\n",
    "    .withColumn('isparking',   col('isparkingspaceincludedinprice').cast('int'))\n",
    "    .withColumn('latitude',    col('latitude').cast('float'))\n",
    "    .withColumn('longitude',   col('longitude').cast('float'))\n",
    "    .withColumn('price',       col('price').cast('float'))\n",
    "    .withColumn('rooms',       col('rooms').cast('int'))\n",
    "    .withColumn('size',        col('size').cast('float'))\n",
    "    .withColumn('year',        col('year').cast('int'))\n",
    "    .withColumn('quarter',     col('quarter').cast('int'))\n",
    ")\n",
    "\n",
    "df_cleaned.printSchema()\n",
    "\n",
    "# 5) Crear vector de características con VectorAssembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "feature_cols = ['bathrooms', 'isparking', 'latitude', 'longitude', 'rooms', 'size', 'year', 'quarter']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol='features',\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "\n",
    "# 6) Transformar DataFrame para obtener 'features' y 'price'\n",
    "df_prepared = assembler.transform(df_cleaned).select('features', 'price')\n",
    "\n",
    "# 7) Dividir en entrenamiento y prueba\n",
    "train_data, test_data = df_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 8) Entrenar modelo de Gradient Boosting\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "xgb = GBTRegressor(featuresCol='features', labelCol='price', maxIter=100, maxDepth=5)\n",
    "\n",
    "with mlflow.start_run(run_name=\"boost_model_v1\") as run:\n",
    "    # Entrenar\n",
    "    xgb_model = xgb.fit(train_data)\n",
    "    \n",
    "    # Predecir\n",
    "    xgb_predictions = xgb_model.transform(test_data)\n",
    "    \n",
    "    # Evaluar con RMSE, MAE y R2\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "    evaluator_mae  = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='mae')\n",
    "    evaluator_r2   = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='r2')\n",
    "    \n",
    "    rmse_xgb = evaluator_rmse.evaluate(xgb_predictions)\n",
    "    mae_xgb  = evaluator_mae.evaluate(xgb_predictions)\n",
    "    r2_xgb   = evaluator_r2.evaluate(xgb_predictions)\n",
    "    \n",
    "    # Registrar métricas en MLflow\n",
    "    mlflow.log_metric(\"rmse_xgb\", rmse_xgb)\n",
    "    mlflow.log_metric(\"mae_xgb\",  mae_xgb)\n",
    "    mlflow.log_metric(\"r2_xgb\",   r2_xgb)\n",
    "    \n",
    "    mlflow.log_param(\"seed\", 42)\n",
    "    mlflow.log_param(\"feature_cols\", feature_cols)\n",
    "    mlflow.log_param(\"maxIter\", 100)\n",
    "    mlflow.log_param(\"maxDepth\", 5)\n",
    "    \n",
    "    # Registrar el modelo\n",
    "    mlflow.spark.log_model(xgb_model, \"gbt-model\")\n",
    "\n",
    "    print(f\"[GBT]  RMSE={rmse_xgb}, MAE={mae_xgb}, R2={r2_xgb}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GXBoost Regression Real Estate",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
