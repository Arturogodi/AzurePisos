{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52839202-0719-469e-aa86-494ee4c9bad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../ConfigFolder/ConfigSAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63cccdc0-b2b0-491f-97e9-a70e0bc8724f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_combined_Path = generate_path('g-model-madrid-min-features-combined', 'goldlayer')\n",
    "\n",
    "df_model_combined = spark.read.format(\"delta\").load(model_combined_Path)\n",
    "\n",
    "display(df_model_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ed972a7-a0cd-4842-9bcf-c677f3825a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "df_model_combined = df_model_combined.withColumn(\"year\", substring(col(\"period\"), 1, 4)) \\\n",
    "                                     .withColumn(\"month\", substring(col(\"period\"), 6, 2))\n",
    "\n",
    "display(df_model_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1572f02f-08fc-4ecd-8aae-73675e194306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_model_combined = df_model_combined.withColumn(\"quarter\", \n",
    "    when(col(\"month\") == \"3\", \"1\")\n",
    "    .when(col(\"month\") == \"6\", \"2\")\n",
    "    .when(col(\"month\") == \"9\", \"3\")\n",
    "    .when(col(\"month\") == \"2\", \"4\")\n",
    ").drop(\"month\", \"period\")\n",
    "\n",
    "display(df_model_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9974520-a0f7-4d15-9d74-73389aacc143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convertir columnas al tipo adecuado\n",
    "df_cleaned = df_model_combined \\\n",
    "    .withColumn('bathrooms', col('bathrooms').cast('int')) \\\n",
    "    .withColumn('isparking', col('isparkingspaceincludedinprice').cast('int')) \\\n",
    "    .withColumn('latitude', col('latitude').cast('float')) \\\n",
    "    .withColumn('longitude', col('longitude').cast('float')) \\\n",
    "    .withColumn('price', col('price').cast('float')) \\\n",
    "    .withColumn('rooms', col('rooms').cast('int')) \\\n",
    "    .withColumn('size', col('size').cast('float')) \\\n",
    "    .withColumn('year', col('year').cast('int')) \\\n",
    "    .withColumn('quarter', col('quarter').cast('int'))\n",
    "\n",
    "# Verifica el esquema para asegurarte de que los tipos de datos sean correctos\n",
    "df_cleaned.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d497919-3744-4088-8355-c973e69e65c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08041792-0127-479b-b622-4d10a4222800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df_combined.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(generate_path('g-model-madrid-min-features-combined', 'goldlayer'))\n",
    "\n",
    "df_filtered2024.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(generate_path('g-model-madrid-min-features-2024', 'goldlayer'))\n",
    "\n",
    "df_selected_2018_renamed.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(generate_path('g-model-madrid-min-features-2018', 'goldlayer'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461f92f0-d527-46e6-a78c-3fa0c8be5b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Definir las columnas a usar como features\n",
    "feature_cols = ['bathrooms', 'isparking', 'latitude', 'longitude', 'rooms', 'size', 'year', 'quarter']\n",
    "\n",
    "# Ensamblar las características en un solo vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features', handleInvalid='skip')\n",
    "\n",
    "# Preparar los datos\n",
    "df_cleaned = df_cleaned.withColumn('isparking', col('isparkingspaceincludedinprice').cast('int'))\n",
    "df_prepared = assembler.transform(df_cleaned).select('features', 'price')\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba (80% / 20%)\n",
    "train_data, test_data = df_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Crear el modelo de regresión lineal\n",
    "lr = LinearRegression(featuresCol='features', labelCol='price', predictionCol='prediction')\n",
    "\n",
    "with mlflow.start_run(run_name=\"regresion_lineal_v1\") as run:\n",
    "    # Entrenar el modelo\n",
    "    lr_model = lr.fit(train_data)\n",
    "    \n",
    "    # Realizar predicciones sobre el conjunto de prueba\n",
    "    predictions = lr_model.transform(test_data)\n",
    "    \n",
    "    # Evaluar el modelo usando RMSE (Raíz del error cuadrático medio)\n",
    "    evaluator = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    \n",
    "    # Registrar las métricas en MLflow\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_param(\"seed\", 42)\n",
    "    mlflow.log_param(\"feature_cols\", feature_cols)\n",
    "    \n",
    "    # Registrar el modelo en MLflow\n",
    "    mlflow.spark.log_model(lr_model, \"linear-regression-model\")\n",
    "    \n",
    "    print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c16cc7-0b45-4260-858f-9533e79fbce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Definir las columnas a usar como features\n",
    "feature_cols = ['bathrooms', 'isparking', 'latitude', 'longitude', 'rooms', 'size', 'year', 'quarter']\n",
    "\n",
    "# Ensamblar las características en un solo vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features', handleInvalid='skip')\n",
    "\n",
    "# Preparar los datos\n",
    "df_cleaned = df_cleaned.withColumn('isparking', col('isparkingspaceincludedinprice').cast('int'))\n",
    "df_prepared = assembler.transform(df_cleaned).select('features', 'price')\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba (80% / 20%)\n",
    "train_data, test_data = df_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Crear el modelo de regresión lineal\n",
    "lr = LinearRegression(featuresCol='features', labelCol='price', predictionCol='prediction')\n",
    "\n",
    "with mlflow.start_run(run_name=\"regresion_lineal_v2\") as run:\n",
    "    # Entrenar el modelo\n",
    "    lr_model = lr.fit(train_data)\n",
    "    \n",
    "    # Realizar predicciones sobre el conjunto de prueba\n",
    "    predictions = lr_model.transform(test_data)\n",
    "    \n",
    "    # Evaluar el modelo usando diferentes métricas\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "    evaluator_mae = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='mae')\n",
    "    evaluator_r2 = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='r2')\n",
    "    \n",
    "    rmse = evaluator_rmse.evaluate(predictions)\n",
    "    mae = evaluator_mae.evaluate(predictions)\n",
    "    r2 = evaluator_r2.evaluate(predictions)\n",
    "    \n",
    "    # Registrar las métricas en MLflow\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_param(\"seed\", 42)\n",
    "    mlflow.log_param(\"feature_cols\", feature_cols)\n",
    "    \n",
    "    # Registrar el modelo en MLflow\n",
    "    mlflow.spark.log_model(lr_model, \"linear-regression-model\")\n",
    "    \n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da111d6a-0a18-40bd-acfb-8bf80132d187",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Decision Tree Regressor"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# Crear el modelo de árbol de decisión\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='price')\n",
    "\n",
    "# Entrenar el modelo\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Realizar predicciones\n",
    "predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Evaluar el modelo\n",
    "evaluator = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b752d83-d705-44c8-8ffe-8451ccfed98a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Random Forest"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Crear el modelo de Random Forest\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='price', numTrees=100)\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Realizar predicciones\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluar el modelo\n",
    "evaluator = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03e7737c-a966-4272-8aef-38c31983f76e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "XGBosst"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Crear el modelo de XGBoost\n",
    "xgb = GBTRegressor(featuresCol='features', labelCol='price', maxIter=100, maxDepth=5)\n",
    "\n",
    "# Entrenar el modelo\n",
    "xgb_model = xgb.fit(train_data)\n",
    "\n",
    "# Realizar predicciones\n",
    "predictions = xgb_model.transform(test_data)\n",
    "\n",
    "# Evaluar el modelo\n",
    "evaluator = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca68eb2d-20b2-4b24-8bc2-4d9c239efaf3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Gradient Boosting"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Crear el modelo de Gradient Boosting\n",
    "gb = GBTRegressor(featuresCol='features', labelCol='price', maxIter=100, maxDepth=5)\n",
    "\n",
    "# Entrenar el modelo\n",
    "gb_model = gb.fit(train_data)\n",
    "\n",
    "# Realizar predicciones\n",
    "predictions = gb_model.transform(test_data)\n",
    "\n",
    "# Evaluar el modelo\n",
    "evaluator = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee5c4ae-e28f-4850-a2f1-08615bb48a3b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "COmparative"
    }
   },
   "outputs": [],
   "source": [
    "# Crear un diccionario para almacenar los resultados\n",
    "results = {}\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "models = [\n",
    "    ('DecisionTree', DecisionTreeRegressor(featuresCol='features', labelCol='price')),\n",
    "    ('RandomForest', RandomForestRegressor(featuresCol='features', labelCol='price', numTrees=100)),\n",
    "    ('XGBoost', GBTRegressor(featuresCol='features', labelCol='price', maxIter=100, maxDepth=5)),\n",
    "    ('GradientBoosting', GBTRegressor(featuresCol='features', labelCol='price', maxIter=100, maxDepth=5))\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    model_model = model.fit(train_data)\n",
    "    predictions = model_model.transform(test_data)\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    results[name] = rmse\n",
    "\n",
    "# Imprimir los resultados\n",
    "for name, rmse in results.items():\n",
    "    print(f\"{name}: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3022cb83-bceb-4a2d-8dfd-47e5be0bb5f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Definir las columnas a usar como features\n",
    "feature_cols = ['bathrooms', 'isparking', 'latitude', 'longitude', 'rooms', 'size', 'year', 'quarter']\n",
    "\n",
    "# Ensamblar las características en un solo vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features', handleInvalid='skip')\n",
    "\n",
    "# Preparar los datos\n",
    "df_cleaned = df_cleaned.withColumn('isparking', col('isparkingspaceincludedinprice').cast('int'))\n",
    "df_prepared = assembler.transform(df_cleaned).select('features', 'price')\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba (80% / 20%)\n",
    "train_data, test_data = df_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Crear un diccionario para almacenar los resultados\n",
    "results = {}\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "models = [\n",
    "    ('DecisionTree', DecisionTreeRegressor(featuresCol='features', labelCol='price')),\n",
    "    ('RandomForest', RandomForestRegressor(featuresCol='features', labelCol='price', numTrees=100)),\n",
    "    ('XGBoost', GBTRegressor(featuresCol='features', labelCol='price', maxIter=100, maxDepth=5))\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    with mlflow.start_run(run_name=name) as run:\n",
    "        # Entrenar el modelo\n",
    "        model_model = model.fit(train_data)\n",
    "        \n",
    "        # Realizar predicciones\n",
    "        predictions = model_model.transform(test_data)\n",
    "        \n",
    "        # Evaluar el modelo\n",
    "        evaluator = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        \n",
    "        # Registrar las métricas en MLflow\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_param(\"seed\", 42)\n",
    "        mlflow.log_param(\"feature_cols\", feature_cols)\n",
    "        \n",
    "        # Registrar el modelo en MLflow\n",
    "        mlflow.spark.log_model(model_model, f\"{name}-model\")\n",
    "        \n",
    "        # Almacenar los resultados\n",
    "        results[name] = rmse\n",
    "\n",
    "# Imprimir los resultados\n",
    "for name, rmse in results.items():\n",
    "    print(f\"{name}: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23986193-b1dd-4b22-bf66-db40ca24774e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68685eb1-0db3-48f3-8d07-e136c68982cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222c17d4-5b3e-42f7-b115-4a2718c7c0f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_model.save(\"DecisionTree-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "496376f5-01e5-4729-8a71-8b68674458aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cargar el modelo\n",
    "model = PipelineModel.load(\"/dbfs/path/to/DecisionTree-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8b22328-8c2a-46c0-8739-8f4ab7c54819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78a831f1-9798-4455-9589-6efbbb5161a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test"
    }
   },
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"Predicción de precios\").getOrCreate()\n",
    "\n",
    "# Cargar el modelo entrenado con una ruta absoluta\n",
    "model = PipelineModel.load(\"/dbfs/path/to/DecisionTree-model\")\n",
    "\n",
    "# Crear un archivo de prueba\n",
    "data_prueba = spark.createDataFrame([\n",
    "    (2, 0, 40.4168, -3.7038, 4, 120, 2022, 3)\n",
    "], [\"bathrooms\", \"isparking\", \"latitude\", \"longitude\", \"rooms\", \"size\", \"year\", \"quarter\"])\n",
    "\n",
    "# Crear un assembler para convertir los datos en un vector\n",
    "assembler = VectorAssembler(inputCols=[\"bathrooms\", \"isparking\", \"latitude\", \"longitude\", \"rooms\", \"size\", \"year\", \"quarter\"], outputCol=\"features\")\n",
    "\n",
    "# Transformar los datos de prueba\n",
    "data_prueba_transformada = assembler.transform(data_prueba)\n",
    "\n",
    "# Hacer predicciones\n",
    "predicciones = model.transform(data_prueba_transformada)\n",
    "\n",
    "# Mostrar las predicciones\n",
    "display(predicciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e16df8df-b819-45e8-918c-aafd05c23d29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cargar el modelo de RandomForest\n",
    "model_rf = PipelineModel.load(\"RandomForest-model\")\n",
    "\n",
    "# Cargar el modelo de XGBoost\n",
    "model_xgb = PipelineModel.load(\"XGBoost-model\")\n",
    "\n",
    "# Hacer predicciones con el modelo de RandomForest\n",
    "predicciones_rf = model_rf.transform(data_prueba_transformada)\n",
    "\n",
    "# Hacer predicciones con el modelo de XGBoost\n",
    "predicciones_xgb = model_xgb.transform(data_prueba_transformada)\n",
    "\n",
    "# Mostrar las predicciones\n",
    "predicciones_rf.show()\n",
    "predicciones_xgb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "626cd82d-73f7-45dc-aef0-9d473bd2f66e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19c1001b-fa13-4449-88d5-feb9b6b51b9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76ff0d46-af41-458a-bf6a-5d5d8cdb1e53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Define el URI del modelo registrado en MLflow\n",
    "MODEL_URI = \"dbfs:/databricks/mlflow-tracking/460952b6c02c4be8aa1a411e99c32040/0d97c667eaea40299b3b5a4a746a8bd2/artifacts/XGBoost-model\"\n",
    "\n",
    "# Descargar el modelo a un directorio local\n",
    "mlflow.artifacts.download_artifacts(artifact_uri=MODEL_URI, dst_path=\"./local_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63fb5d79-929d-4eed-b6d2-2d5b2b842af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models import validate_serving_input\n",
    "from mlflow.models.utils import _enforce_schema\n",
    "\n",
    "model_uri = 'runs:/0d97c667eaea40299b3b5a4a746a8bd2/XGBoost-model'\n",
    "\n",
    "# Define INPUT_EXAMPLE via assignment with your own input example to the model\n",
    "# A valid input example is a data instance suitable for pyfunc prediction\n",
    "INPUT_EXAMPLE = {\n",
    "    # Add your input example here\n",
    "}\n",
    "\n",
    "# Convert input example to serving input\n",
    "serving_payload = _enforce_schema(INPUT_EXAMPLE)\n",
    "\n",
    "# Validate the serving payload works on the model\n",
    "validate_serving_input(model_uri, serving_payload)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Linear Regression Model on Madrid Real Estate Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
