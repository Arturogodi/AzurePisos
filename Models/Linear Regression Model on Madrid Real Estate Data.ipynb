{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52839202-0719-469e-aa86-494ee4c9bad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../ConfigFolder/ConfigSAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afe14b90-fccd-4de4-bcfc-e181eb4d414b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Notebook Description\n",
    "\n",
    "This notebook provides an analysis of the customer churn dataset, including data cleaning, exploratory data analysis, and model building. The goal is to gain insights and build predictive models, such as logistic regression and random forest, to solve the given problem. Additionally, MLflow is used for experiment tracking and model management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63cccdc0-b2b0-491f-97e9-a70e0bc8724f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_combined_Path = generate_path('g-model-madrid-min-features-combined', 'goldlayer')\n",
    "\n",
    "df_model_combined = spark.read.format(\"delta\").load(model_combined_Path)\n",
    "\n",
    "display(df_model_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ed972a7-a0cd-4842-9bcf-c677f3825a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "# Extract the year from the 'period' column\n",
    "df_model_combined = df_model_combined.withColumn(\"year\", substring(col(\"period\"), 1, 4)) \\\n",
    "                                     # Extract the month from the 'period' column\n",
    "                                     .withColumn(\"month\", substring(col(\"period\"), 6, 2))\n",
    "\n",
    "display(df_model_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1572f02f-08fc-4ecd-8aae-73675e194306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Create a new column 'quarter' based on the 'month' column values\n",
    "df_model_combined = df_model_combined.withColumn(\"quarter\", \n",
    "    when(col(\"month\") == \"3\", \"1\")\n",
    "    .when(col(\"month\") == \"6\", \"2\")\n",
    "    .when(col(\"month\") == \"9\", \"3\")\n",
    "    .when(col(\"month\") == \"2\", \"4\")\n",
    ").drop(\"month\", \"period\")  # Drop the 'month' and 'period' columns\n",
    "\n",
    "display(df_model_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9974520-a0f7-4d15-9d74-73389aacc143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert columns to the appropriate type\n",
    "df_cleaned = df_model_combined \\\n",
    "    .withColumn('bathrooms', col('bathrooms').cast('int')) \\\n",
    "    .withColumn('isparking', col('isparkingspaceincludedinprice').cast('int')) \\\n",
    "    .withColumn('latitude', col('latitude').cast('float')) \\\n",
    "    .withColumn('longitude', col('longitude').cast('float')) \\\n",
    "    .withColumn('price', col('price').cast('float')) \\\n",
    "    .withColumn('rooms', col('rooms').cast('int')) \\\n",
    "    .withColumn('size', col('size').cast('float')) \\\n",
    "    .withColumn('year', col('year').cast('int')) \\\n",
    "    .withColumn('quarter', col('quarter').cast('int'))\n",
    "\n",
    "# Verify the schema to ensure the data types are correct\n",
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d497919-3744-4088-8355-c973e69e65c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08041792-0127-479b-b622-4d10a4222800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df_combined.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(generate_path('g-model-madrid-min-features-combined', 'goldlayer'))\n",
    "\n",
    "df_filtered2024.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(generate_path('g-model-madrid-min-features-2024', 'goldlayer'))\n",
    "\n",
    "df_selected_2018_renamed.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(generate_path('g-model-madrid-min-features-2018', 'goldlayer'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461f92f0-d527-46e6-a78c-3fa0c8be5b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Define the columns to use as features\n",
    "feature_cols = ['bathrooms', 'isparking', 'latitude', 'longitude', 'rooms', 'size', 'year', 'quarter']\n",
    "\n",
    "# Assemble the features into a single vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features', handleInvalid='skip')\n",
    "\n",
    "# Prepare the data\n",
    "df_cleaned = df_cleaned.withColumn('isparking', col('isparkingspaceincludedinprice').cast('int'))\n",
    "df_prepared = assembler.transform(df_cleaned).select('features', 'price')\n",
    "\n",
    "# Split the data into training and test sets (80% / 20%)\n",
    "train_data, test_data = df_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create the linear regression model\n",
    "lr = LinearRegression(featuresCol='features', labelCol='price', predictionCol='prediction')\n",
    "\n",
    "with mlflow.start_run(run_name=\"regresion_lineal_v1\") as run:\n",
    "    # Train the model\n",
    "    lr_model = lr.fit(train_data)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    predictions = lr_model.transform(test_data)\n",
    "    \n",
    "    # Evaluate the model using RMSE (Root Mean Squared Error)\n",
    "    evaluator = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    \n",
    "    # Log metrics in MLflow\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_param(\"seed\", 42)\n",
    "    mlflow.log_param(\"feature_cols\", feature_cols)\n",
    "    \n",
    "    # Log the model in MLflow\n",
    "    mlflow.spark.log_model(lr_model, \"linear-regression-model\")\n",
    "    \n",
    "    print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c16cc7-0b45-4260-858f-9533e79fbce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Define the columns to use as features\n",
    "feature_cols = ['bathrooms', 'isparking', 'latitude', 'longitude', 'rooms', 'size', 'year', 'quarter']\n",
    "\n",
    "# Assemble the features into a single vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features', handleInvalid='skip')\n",
    "\n",
    "# Prepare the data\n",
    "df_cleaned = df_cleaned.withColumn('isparking', col('isparkingspaceincludedinprice').cast('int'))\n",
    "df_prepared = assembler.transform(df_cleaned).select('features', 'price')\n",
    "\n",
    "# Split the data into training and test sets (80% / 20%)\n",
    "train_data, test_data = df_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create the linear regression model\n",
    "lr = LinearRegression(featuresCol='features', labelCol='price', predictionCol='prediction')\n",
    "\n",
    "with mlflow.start_run(run_name=\"regresion_lineal_v2\") as run:\n",
    "    # Train the model\n",
    "    lr_model = lr.fit(train_data)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    predictions = lr_model.transform(test_data)\n",
    "    \n",
    "    # Evaluate the model using different metrics\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "    evaluator_mae = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='mae')\n",
    "    evaluator_r2 = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='r2')\n",
    "    \n",
    "    rmse = evaluator_rmse.evaluate(predictions)\n",
    "    mae = evaluator_mae.evaluate(predictions)\n",
    "    r2 = evaluator_r2.evaluate(predictions)\n",
    "    \n",
    "    # Log metrics in MLflow\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_param(\"seed\", 42)\n",
    "    mlflow.log_param(\"feature_cols\", feature_cols)\n",
    "    \n",
    "    # Log the model in MLflow\n",
    "    mlflow.spark.log_model(lr_model, \"linear-regression-model\")\n",
    "    \n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da111d6a-0a18-40bd-acfb-8bf80132d187",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Decision Tree Regressor"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# Create the decision tree model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='price')\n",
    "\n",
    "# Train the model\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b752d83-d705-44c8-8ffe-8451ccfed98a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Random Forest"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Create the Random Forest model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='price', numTrees=100)\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03e7737c-a966-4272-8aef-38c31983f76e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "XGBosst"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Create the XGBoost model\n",
    "xgb = GBTRegressor(featuresCol='features', labelCol='price', maxIter=100, maxDepth=5)\n",
    "\n",
    "# Train the model\n",
    "xgb_model = xgb.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = xgb_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca68eb2d-20b2-4b24-8bc2-4d9c239efaf3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Gradient Boosting"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Create the Gradient Boosting model\n",
    "gb = GBTRegressor(featuresCol='features', labelCol='price', maxIter=100, maxDepth=5)\n",
    "\n",
    "# Train the model\n",
    "gb_model = gb.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = gb_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee5c4ae-e28f-4850-a2f1-08615bb48a3b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "COmparative"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "models = [\n",
    "    ('DecisionTree', DecisionTreeRegressor(featuresCol='features', labelCol='price')),\n",
    "    ('RandomForest', RandomForestRegressor(featuresCol='features', labelCol='price', numTrees=100)),\n",
    "    ('XGBoost', GBTRegressor(featuresCol='features', labelCol='price', maxIter=100, maxDepth=5)),\n",
    "    ('GradientBoosting', GBTRegressor(featuresCol='features', labelCol='price', maxIter=100, maxDepth=5))\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    model_model = model.fit(train_data)\n",
    "    predictions = model_model.transform(test_data)\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    results[name] = rmse\n",
    "\n",
    "# Print the results\n",
    "for name, rmse in results.items():\n",
    "    print(f\"{name}: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3022cb83-bceb-4a2d-8dfd-47e5be0bb5f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Define the columns to use as features\n",
    "feature_cols = ['bathrooms', 'isparking', 'latitude', 'longitude', 'rooms', 'size', 'year', 'quarter']\n",
    "\n",
    "# Assemble the features into a single vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features', handleInvalid='skip')\n",
    "\n",
    "# Prepare the data\n",
    "df_cleaned = df_cleaned.withColumn('isparking', col('isparkingspaceincludedinprice').cast('int'))\n",
    "df_prepared = assembler.transform(df_cleaned).select('features', 'price')\n",
    "\n",
    "# Split the data into training and testing sets (80% / 20%)\n",
    "train_data, test_data = df_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "models = [\n",
    "    ('DecisionTree', DecisionTreeRegressor(featuresCol='features', labelCol='price')),\n",
    "    ('RandomForest', RandomForestRegressor(featuresCol='features', labelCol='price', numTrees=100)),\n",
    "    ('XGBoost', GBTRegressor(featuresCol='features', labelCol='price', maxIter=100, maxDepth=5))\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    with mlflow.start_run(run_name=name) as run:\n",
    "        # Train the model\n",
    "        model_model = model.fit(train_data)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model_model.transform(test_data)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        evaluator = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        \n",
    "        # Log metrics in MLflow\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_param(\"seed\", 42)\n",
    "        mlflow.log_param(\"feature_cols\", feature_cols)\n",
    "        \n",
    "        # Log the model in MLflow\n",
    "        mlflow.spark.log_model(model_model, f\"{name}-model\")\n",
    "        \n",
    "        # Store the results\n",
    "        results[name] = rmse\n",
    "\n",
    "# Print the results\n",
    "for name, rmse in results.items():\n",
    "    print(f\"{name}: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222c17d4-5b3e-42f7-b115-4a2718c7c0f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_model.save(\"DecisionTree-model\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Linear Regression Model on Madrid Real Estate Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
